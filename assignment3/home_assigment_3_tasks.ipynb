{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "080be2e1",
   "metadata": {},
   "source": [
    "Text Analytics I HWS 23/24\n",
    "\n",
    "# Home Assignment 3 (30pts)\n",
    "\n",
    "Submit your solution via Ilias until 23.59h on Wednesday, November 8th. Late submissions are accepted until 12:00am on the following day, with 1/4 of the total possible points deducted from the score.\n",
    "\n",
    "Submit your solutions in teams of 3-4 students. Unless explicitly agreed otherwise in advance, **submissions from teams with more or less members will NOT be graded**.\n",
    "List all members of the team with their student ID in the cell below, and submit only one notebook per team. Only submit a notebook, do not submit the dataset(s) you used. Also, do NOT compress/zip your submission!\n",
    "\n",
    "You may use the code from the exercises and basic functionalities that are explained in official documentation of Python packages without citing, __all other sources must be cited__. In case of plagiarism (copying solutions from other teams or from the internet) ALL team members may be expelled from the course without warning.\n",
    "\n",
    "#### General guidelines:\n",
    "* Make sure that your code is executable, any task for which the code does not directly run on our machine will be graded with 0 points.\n",
    "* If you use packages that are not available on the default or conda-forge channel, list them below. Also add a link to installation instructions. \n",
    "* Ensure that the notebook does not rely on the current notebook or system state!\n",
    "  * Use `Kernel --> Restart & Run All` to see if you are using any definitions, variables etc. that \n",
    "    are not in scope anymore.\n",
    "  * Do not rename any of the datasets you use, and load it from the same directory that your ipynb-notebook is located in, i.e., your working directory.\n",
    "* Make sure you clean up your code before submission, e.g., properly align your code, and delete every line of code that you do not need anymore, even if you may have experimented with it. Minimize usage of global variables. Avoid reusing variable names multiple times!\n",
    "* Ensure your code/notebook terminates in reasonable time.\n",
    "* Feel free to use comments in the code. While we do not require them to get full marks, they may help us in case your code has minor errors.\n",
    "* For questions that require a textual answer, please do not write the answer as a comment in a code cell, but in a Markdown cell below the code. Always remember to provide sufficient justification for all answers.\n",
    "* You may create as many additional cells as you want, just make sure that the solutions to the individual tasks can be found near the corresponding assignment.\n",
    "* If you have any general question regarding the understanding of some task, do not hesitate to post in the student forum in Ilias, so we can clear up such questions for all students in the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "275631a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# studentIDs of all team members\n",
    "team_members = [1966868,1967897, 1968154, 1978986, 1951865]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ed9164",
   "metadata": {},
   "source": [
    "Additional packages (if any):\n",
    " - Example: `powerlaw`, https://github.com/jeffalstott/powerlaw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36ef36f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Union, Dict, Set, Tuple\n",
    "from numpy.typing import NDArray\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acc6b92",
   "metadata": {},
   "source": [
    "### Task 1: POS tagging (6 points)\n",
    "\n",
    "In this task, we want to explore sentences with similar part of speech (POS) tag structure. For this, we need a corpus of text with tags. We will generate such a corpus by using NLTKâ€™s currently recommended POS tagger to tag a given list of tokens (https://www.nltk.org/api/nltk.tag.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01d59e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK's off-the-shelf POS tagger\n",
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e78bb0d",
   "metadata": {},
   "source": [
    "__a)__ Given a corpus of text ``corpus`` as a sequence of tokens, we want to collect all words that are tagged with a certain POS tag. Implement a function ``collect_words_for_tag`` that first tags the given corpus using NLTK's off-the-shelf tagger imported in the cell above. Then, for each POS tag, collect all words that were tagged with it. You should return a dictionary that maps each POS tag that was observed to the set of words that were assigned this tag in the given corpus. __(2 pts)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5651149b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'set'>, {'DT': {'a', 'This'}, 'VBZ': {'is'}, 'JJ': {'simple'}, 'NN': {'test'}})\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from nltk.corpus.reader.util import StreamBackedCorpusView \n",
    "from nltk.tokenize import word_tokenize\n",
    "# nltk.download('averaged_perceptron_tagger') requires this download\n",
    "\n",
    "\n",
    "def collect_words_for_tag(corpus: Union[List[str], StreamBackedCorpusView]) -> Dict[str, Set[str]]:\n",
    "    '''\n",
    "    :param corpus: sequence of tokens that represents the text corpus\n",
    "    :return: dict that maps each tag to a set of tokens that were assigned this tag in the corpus\n",
    "    '''\n",
    "    tags = defaultdict(set)\n",
    "    for token, tag in pos_tag(corpus):\n",
    "        tags[tag].add(token)\n",
    "    return tags\n",
    "\n",
    "test = collect_words_for_tag(word_tokenize(\"This is a simple test\"))\n",
    "print(test) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba737321",
   "metadata": {},
   "source": [
    "__b)__ Implement a function ``generate_sentences`` that gets a sentence and a POS dictionary (assume the POS dictionary was generated by your function in __a)__) as input and generates ``n`` sequences of words with the same tag structure. The words in your generated sequence should be randomly taken from the set of words associated with the current tag. \n",
    "\n",
    "Additionally, the user should have the option to achieve sentences of ``better_quality``. Thus, if ``better_quality=True``, make sure that the tag structure of the output sentences actually matches the tag structure of the input sentence, as the tags may change depending on the context. \n",
    "\n",
    "You can assume that the training corpus is large enough to include all possible POS tags. __(2 pts)__\n",
    "\n",
    "_Hint: consider the_ ``random`` _module_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b4efad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def generate_rand(sentence: List[str], pos_dict: Dict[str, Set[str]], n: int, better_quality: bool=False) -> List[List[str]]:\n",
    "    '''\n",
    "    :param sentence: input sentence that sets the tag pattern\n",
    "    :param pos_dict: maps each tag to a list of associated words\n",
    "    :param n: number of sentences that should be generated\n",
    "    :return: List of sentences with the same tag structure as the input sentence\n",
    "    '''\n",
    "    \n",
    "    # create a list of tags from the input sentence\n",
    "    tags = [tag for _, tag in pos_tag(sentence)]\n",
    "\n",
    "    # create a list of sentences\n",
    "    sentences = []\n",
    "    for _ in range(n):\n",
    "        # create a list of sentences\n",
    "        new_sentance = [random.choice(list(pos_dict[t])) for t in tags]\n",
    "        if (better_quality):\n",
    "            new_tags = [tag for _, tag in pos_tag(new_sentance)]\n",
    "            while new_tags != tags:\n",
    "                new_sentance = [random.choice(list(pos_dict[t])) for t in tags]\n",
    "                new_tags = [tag for _, tag in pos_tag(sentence)]\n",
    "\n",
    "        sentences.append(new_sentance)\n",
    "\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a9b3ba",
   "metadata": {},
   "source": [
    "__c)__ Using the input sentence ``This test is very difficult``, test your implementation to generate 10 sentences based on  \n",
    "\n",
    "* \"Emma\" by Jane Austen\n",
    "\n",
    "* The \"King James Bible\"\n",
    "\n",
    "Store your POS dictionary in ``emma_tags``and ``bible_tags``, respectively. Your generated sentences should be stored in ``emma_sent`` and ``bible_sent``. __(2 pts)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a69ab118",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = [\"This\", \"test\", \"is\", \"very\", \"difficult\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64678c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "emma_jane_austen = nltk.corpus.gutenberg.words('austen-emma.txt')\n",
    "king_james_bible = nltk.corpus.gutenberg.words('bible-kjv.txt')\n",
    "\n",
    "emma_tags = collect_words_for_tag(emma_jane_austen)\n",
    "king_tags = collect_words_for_tag(king_james_bible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad042eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['either', 'improper', 'changes', 'thoughtfully', 'nephews']\n",
      "['a', 'carriage', 'means', '\"', 'argument']\n",
      "['every', 'poverty', 'saves', 'accordingly', '_any_']\n",
      "['this', 'penitence', 'is', 'openly', 'sudden']\n",
      "['both', 'guinea', 'plays', 'agreeably', 'depressed']\n",
      "['these', '_purport_', 'sets', 'later', 'destin']\n",
      "['all', 'eating', 'passes', 'formerly', 'entire']\n",
      "['loth', 'grandeur', 'awes', 'Absolutely', 'frozen']\n",
      "['some', 'repugnance', 'sposo', 'composedly', 'honour']\n",
      "['Every', 'scholar', 'turns', 'inexpressibly', 'untainted']\n"
     ]
    }
   ],
   "source": [
    "emma_sent = generate_rand(sent, emma_tags, 10, better_quality=True)\n",
    "king_sent = generate_rand(sent, king_tags, 10)\n",
    "\n",
    "for s in emma_sent:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beecad4e",
   "metadata": {},
   "source": [
    "### Task 2: The Viterbi algorithm (12 points)\n",
    "Implement the Viterbi algorithm as introduced in the lecture (lecture 8, slide 20) and the exercise. The input of your function is a sentence that should be tagged, a dictionary with state transition probabilites and a dictionary with word emission probabilities. You may assume that the _transition probabilities_ are complete, i.e. the dictionary includes every combination of states. In contrast, we assume that all combinations of words and POS tags that are not in the dictionary of _emission probabilities_ have an emission probability of 0.\n",
    "\n",
    "The function should return a list of POS tags, s.t. that each tag corresponds to a word of the input sentence. Moreover, return the probability of the sequence of POS tags that you found. \n",
    "\n",
    "You can test your function on the given example that was discussed in the Pen&Paper exercise. For the sentence ``the fans watch the race`` and the provided probabilities, your function should return the POS tag sequence ``['DT', 'N', 'V', 'DT', 'N']`` and a probability of ``9.720000000000002e-06``.\n",
    "\n",
    "Additionally, implement beam search in the viterbi algorithm. The beam size is defined by the parameter `beam`. For example for `beam=2` we only keep the best 2 scores per column in each step and discard the rest. You may use the example from the lecture to test your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8319309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test sentence\n",
    "sentence = [\"the\", \"fans\", \"watch\", \"the\", \"race\"]\n",
    "\n",
    "# state transition probabilities (complete)\n",
    "state_trans_prob = {('<s>','DT'):0.8,('<s>','N'):0.2,('<s>','V'):0.0,\n",
    "                    ('DT','DT'):0.0,('DT','N'):0.9,('DT','V'):0.1,\n",
    "                    ('N','DT'):0.0,('N','N'):0.5,('N','V'):0.5,\n",
    "                    ('V','DT'):0.5,('V','N'):0.5,('V','V'):0.0}\n",
    "\n",
    "# word emission probabilities (not complete, all combinations that are not present have probability 0)\n",
    "word_emission_prob = {('the','DT'):0.2, ('fans','N'):0.1,('fans','V'):0.2,('watch','N'):0.3,\n",
    "                      ('watch','V'):0.15,('race','N'):0.1,('race','V'):0.3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9404d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DT', 'N', 'V', 'DT', 'N']\n",
      "9.720000000000002e-06\n",
      "--- 0.0009989738464355469 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "import numpy as np\n",
    "def Viterbi(sentence: List[str], trans_prob: Dict[Tuple[str,str], float], emiss_prob: Dict[Tuple[str,str], float], beam: int=0) -> (List[str], float):\n",
    "    '''\n",
    "    :param sentence: sentence that we want to tag\n",
    "    :param trans_prob: dict with state transition probabilities\n",
    "    :param emiss_prob: dict with word emission probabilities\n",
    "    :param beam: beam size for beam search. If 0, don't apply beam search\n",
    "    :returns: \n",
    "        - list with POS tags for each input word\n",
    "        - float that indicates the probability of the tag sequence\n",
    "    '''\n",
    "    all_possible_pos_tags = set([tag for _, tag in emiss_prob.keys()])\n",
    "    viterbi = np.zeros(\n",
    "        shape=(len(all_possible_pos_tags), len(sentence)), dtype=float)\n",
    "    backpointer = np.zeros(\n",
    "        shape=(len(all_possible_pos_tags), len(sentence)), dtype=int)\n",
    "    \n",
    "    # initialize first column\n",
    "    for i, tag in enumerate(all_possible_pos_tags):\n",
    "        viterbi[i,0] = trans_prob[('<s>', tag)] * emiss_prob.get((sentence[0], tag), 0)\n",
    "        # backpointer[i,0] = 0\n",
    "\n",
    "    # Recursion step\n",
    "    for t in range(1,len(sentence)):\n",
    "        for i, tag in enumerate(all_possible_pos_tags):\n",
    "            if beam == 0:\n",
    "                max_prob = -1\n",
    "                max_prob_tag = None\n",
    "                for j, prev_tag in enumerate(all_possible_pos_tags):\n",
    "                    prob = viterbi[j, t-1] * trans_prob.get((prev_tag, tag), 0) * emiss_prob.get((sentence[t], tag), 0)\n",
    "                    if prob > max_prob:\n",
    "                        max_prob = prob\n",
    "                        max_prob_tag = prev_tag\n",
    "                viterbi[i, t] = max_prob\n",
    "                backpointer[i, t] = list(all_possible_pos_tags).index(max_prob_tag)\n",
    "            else:\n",
    "                top_k_probs = []\n",
    "                top_k_tags = []\n",
    "                for j, prev_tag in enumerate(all_possible_pos_tags):\n",
    "                    prob = viterbi[j, t-1] * trans_prob.get((prev_tag, tag), 0) * emiss_prob.get((sentence[t], tag), 0)\n",
    "                    if prob > 0:\n",
    "                        top_k_probs.append(prob)\n",
    "                        top_k_tags.append(j)\n",
    "                if len(top_k_probs) > beam:\n",
    "                    top_k_probs, top_k_tags = zip(*sorted(zip(top_k_probs, top_k_tags), reverse=True)[:beam])\n",
    "                for k, prob in enumerate(top_k_probs):\n",
    "                    viterbi[k, t] = prob\n",
    "                    backpointer[k, t] = top_k_tags[k]\n",
    "\n",
    "\n",
    "    # Termination step\n",
    "    best_path_prob = np.max(viterbi[:, t])\n",
    "    best_path_pointer = np.argmax(viterbi[:, t])\n",
    "    best_path = [best_path_pointer]\n",
    "    for t in range(len(sentence)-1, 0, -1):\n",
    "        best_path_pointer = backpointer[best_path_pointer, t]\n",
    "        best_path.append(best_path_pointer)\n",
    "    best_path.reverse()\n",
    "    best_path = [list(all_possible_pos_tags)[i] for i in best_path]\n",
    "    return best_path, best_path_prob\n",
    "\n",
    "\n",
    "best_path, best_path_prob = Viterbi(sentence, state_trans_prob, word_emission_prob, beam=0)\n",
    "print(best_path)\n",
    "print(best_path_prob) \n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c38fe7",
   "metadata": {},
   "source": [
    "### Task 3: ML Basics - Naive Bayes Classification (10pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3ab66f",
   "metadata": {},
   "source": [
    "### Task 3: ML Basics - Naive Bayes Classification (12pts)\n",
    "In this task, we want to build a Naive Bayes classifier with add-1 smoothing for text classification (pseudocode given below), e.g., to assign a category to a document. Use the class-skeleton provided below for your implementation.\n",
    "\n",
    "#### Naive Bayes Pseudocode\n",
    "##### TrainMultiNomialNB($\\mathbb C$,$\\mathbb D$)  \n",
    "$V \\leftarrow extractVocabulary(\\mathbb D)$  \n",
    "$N \\leftarrow countDocs(\\mathbb D)$    \n",
    "for $c \\in \\mathbb C$:  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;$N_c \\leftarrow countDocsInClass(\\mathbb D, c)$  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;$prior[c] \\leftarrow \\frac{N_c}{N}$  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;$text_c \\leftarrow concatenateTextOfAllDocsInClass(\\mathbb D, c)$   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;for $t \\in V$:  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$T_{ct} \\leftarrow countTokensOfTerm(text_c,t)$  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;for $t \\in V$:  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$condprob[t][c] \\leftarrow \\frac{T_{ct} + 1}{\\sum_{t'}(T_{ct'} + 1)}$  \n",
    "return $V,prior,condprob$\n",
    "\n",
    "##### ApplyMultinomialNB($\\mathbb C,V,prior,condprob,d$)\n",
    "$W \\leftarrow extractTokensFromDoc(V,d)$   \n",
    "for $c \\in \\mathbb C$:  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;$score[c] \\leftarrow log(prior[c])$  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;for $t \\in W$:  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$score[c] += log(condprob[t][c])$  \n",
    "return $argmax_{c \\in \\mathbb C} score[c]$\n",
    "\n",
    "__a) Tokenization (1pt)__  \n",
    "Implement the function `tokenize` to transform a text document to a list of tokens with the regex pattern `\\b\\w\\w+\\b`. Transform all tokens to lowercase.\n",
    "\n",
    "__b) Naive Bayes \"Training\" (6pts)__  \n",
    "Implement the `__init__` function to set up the Naive Bayes Model. Cf. TrainMultiNomialNB($\\mathbb C$,$\\mathbb D$) in the pseudocode above. Contrary to the pseudocode, the `__init__` function should not return anything, but the vocabulary, priors and conditionals should be stored in class variables. We only want to keep tokens with a frequeny > `min_count` in the vocabulary.\n",
    "\n",
    "__c) Naive Bayes Classification (3pts)__  \n",
    "Implement the `classify` function to return the most probable class for the provided document according to your Naive Bayes model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f9d9056",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "class NaiveBayesClassifier:\n",
    "    '''Naive Bayes for text classification.'''\n",
    "\n",
    "\n",
    "    def __init__(self, docs: List[str], labels: List[int], min_count: int=1):\n",
    "        '''\n",
    "        :param docs: list of documents from which to build the model (corpus)\n",
    "        :param labels: list of classes assigned to the list of documents (labels[i] is the class for docs[i])\n",
    "        :param min_count: minimum frequency of token in vocabulary (tokens that occur less times are discarded)\n",
    "        '''\n",
    "        self.docs_tokens = [self.tokenize(doc) for doc in docs]\n",
    "        self.V = set([token for doc in self.docs_tokens for token in doc])\n",
    "\n",
    "        # remove tokens that occur less than min_count times\n",
    "        token_counts = Counter([token for doc in self.docs_tokens for token in doc])\n",
    "        self.V = set([token for token in self.V if token_counts[token] > min_count])\n",
    "        # if we include tokens with the mincount we get the same result at sklearns NBClassifier\n",
    "        \n",
    "        N = len(self.docs_tokens)\n",
    "        classes = set(labels)\n",
    "        # compute prior probabilities\n",
    "        self.prior = Counter(labels)\n",
    "        self.conditional = defaultdict(dict)\n",
    "        for c in classes:\n",
    "            self.prior[c] /= N\n",
    "\n",
    "            all_doc_c = [doc for doc, label in zip(self.docs_tokens, labels) if label == c]\n",
    "\n",
    "            all_text_c_counts = Counter(token for doc in all_doc_c for token in doc)\n",
    "            sum_all_text_c_counts = sum(all_text_c_counts.values())\n",
    "\n",
    "            for token in self.V:\n",
    "                self.conditional[c][token] = (all_text_c_counts[token] +1) / (sum_all_text_c_counts + len(self.V))\n",
    "                \n",
    "    def tokenize(self, doc: str):\n",
    "        '''\n",
    "        :param doc: document to tokenize\n",
    "        :return: document as a list of tokens\n",
    "        '''\n",
    "        return re.findall(r'\\b\\w\\w+\\b', doc.lower())\n",
    "        \n",
    "\n",
    "    def classify(self, doc: str):\n",
    "        '''\n",
    "        :param doc: document to classify\n",
    "        :return: most probable class\n",
    "        '''\n",
    "        # your code for Task 3c) here\n",
    "        tokens = self.tokenize(doc)\n",
    "        tokens = [token for token in tokens if token in self.V]\n",
    "        probs = {}\n",
    "        for c in self.prior.keys():\n",
    "            probs[c] = np.log(self.prior[c])\n",
    "            for token in tokens:\n",
    "                probs[c] += np.log(self.conditional[c][token])\n",
    "        # print(probs)\n",
    "        return max(probs.items(), key=lambda x: x[1])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bca1195",
   "metadata": {},
   "source": [
    "__d) Evaluation (2pts)__\n",
    "Test your implementation on the 20newsgroups dataset. If implemented correctly, with `min_count=1` your Naive Bayes classifier should obtain the same accuracy as the implementation by scikit-learn (see below for comparison)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8498f31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html for details\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "train = fetch_20newsgroups(subset='train')\n",
    "test = fetch_20newsgroups(subset='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f166a6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "model = NaiveBayesClassifier(train.data, train.target)\n",
    "print(model.classify(\"God is love\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aea57aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7939458311205523\n"
     ]
    }
   ],
   "source": [
    "accuracy = 0\n",
    "for i, doc in enumerate(test.data):\n",
    "    if model.classify(doc) == test.target[i]:\n",
    "        accuracy += 1\n",
    "accuracy /= len(test.data)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we include tokens with the mincount we get the same result at sklearns NBClassifier.\n",
    "\n",
    "token_counts[token] > min_count results in 0.79\n",
    "token_counts[token] >= min_count results in 0.77\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bcb0288b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7728359001593202"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "test = fetch_20newsgroups(subset='test')\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "x = vectorizer.fit_transform(train.data)\n",
    "clf = MultinomialNB()\n",
    "clf.fit(x,train.target)\n",
    "\n",
    "pred = clf.predict(vectorizer.transform(test.data))\n",
    "\n",
    "accuracy_score(test.target,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
